ica-reconstruction error
-reconstructed
pca-cumulative ratio/explained variance
- explained variance ratio
-reconstruction error
-reconstructed
rp-square distances
-# of dimensions


pca: 
eigenvalue = pca.explained_variance_
eigenvector = pca.components_

How did you choose k?
KM=highest score
GM=highest score
a description of the kind of clusters that you got.
plot clusters
Why did you get the clusters you did? 
Do they make "sense"? 

If you used data that already had labels did the
clusters line up with the labels? 
Do they otherwise line up naturally? 
Why or why not? 
Compare and contrast the different algorithms. 
What sort of changes might you make to each of those algorithms to
improve performance? 
How much performance was due to the problems you chose? 
Be creative and think of as many questions you can, and as many answers as you can. 
Take care to justify your analysis with data explictly.

Can you describe how the data look in the new spaces you created with the various aglorithms? 
For PCA, what is the distribution of eigenvalues? 
For ICA, how kurtotic are the distributions? 
Do the projection axes for ICA seem to capture anything "meaningful"? 
Assuming you only generate k projections (i.e., you do dimensionality reduction), how well is the data reconstructed by the
randomized projections? 
PCA? 

2. RP:
How much variation did you get when you reran your RP several times
(you might want to run RP many times to see what happens)

3. When you reproduced your clustering experiments on the datasets projected onto the new spaces
created by ICA, PCA and RP, did you get the same clusters as before? Different clusters? Why? Why
not?

4, 5.
When you reran your neural network algorithms were there any differences in performance? Speed?
Anything at all?

ICA use kurtosis to evaluate
use some score to pick k

inter/intracluster distance